project_name: PatchSetsRL
experiment_name: Default

debug: False
gpu: null

hparams:
  total_steps_entropy_coef: 0.2
  start_entropy_coef: 1e-0
  end_entropy_coef: 1e-3
  patch_size:
    x: 25
    y: 25
  batch_size: 64

patch_set_buffer:
  use_n_steps: 10

env:
  patch_size: ${hparams.patch_size}
  done_threshold: 0.99

env_model:
  # checkpoint_path: /workspace/src/epoch=1062.ckpt
  output_n: 26
  pool_mode: max

env_model_trainer:
  batch_size: ${hparams.batch_size}
  epochs: 1

agent_model:
  input_n: 2
  patch_size: ${hparams.patch_size}

agent:
  gpu: ${gpu}
  gamma: 0.8
  update_interval: 1024
  minibatch_size: ${hparams.batch_size}
  epochs: 10

experiment:
  steps: 500_000
  eval_interval: ${agent.update_interval}
  eval_n_episodes: 104
  eval_n_steps: ~
  train_max_episode_len: 8
  eval_max_episode_len: 8
  use_tensorboard: True

optim:
  env:
    lr: 1e-3
    eps: 1e-8
  agent:
    lr: 2e-4
    eps: 1e-8

dataset:
  train:
    root: '/dataset/AdobeFontCharImages/splitted'
    data_type:
      - 'train_data_0'
      - 'train_data_1'
      - 'train_data_2'
      - 'train_data_3'
    upper: True
    lower: False
  valid:
    root: '/dataset/AdobeFontCharImages/splitted'
    data_type:
      - 'train_data_4'
    upper: True
    lower: False
  test:
    root: '/dataset/AdobeFontCharImages/splitted'
    data_type:
      - 'test_data'
    upper: True
    lower: False

hydra:
  run:
    dir: ./outputs/${experiment_name}/${now:%Y-%m-%d}/${now:%H-%M-%S}
  sweep:
    dir: ./outputs/${experiment_name}/${now:%Y-%m-%d}/${now:%H-%M-%S}
